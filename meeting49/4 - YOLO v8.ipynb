{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLZD/0PVL+9nMW0VnRob9o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Install required libraries"],"metadata":{"id":"rQZeE3080Cy8"}},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"id":"VZ4ebUua0FKT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load YOLO v8"],"metadata":{"id":"MxrebUla0JA3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bAwU84iz5Iq"},"outputs":[],"source":["from ultralytics import YOLO\n","import cv2\n","\n","# Load a pre-trained YOLOv8 model\n","model = YOLO('yolov8n.pt') # 'n' is for the nano version, the smallest and fastest\n","\n"]},{"cell_type":"code","source":["\n","\n","# Load other pre-trained YOLOv8 models\n","\n","# Load the small version of YOLOv8\n","# model_s = YOLO('yolov8s.pt')\n","\n","# Load the medium version of YOLOv8\n","# model_m = YOLO('yolov8m.pt')\n","\n","# Load the large version of YOLOv8\n","# model_l = YOLO('yolov8l.pt')\n","\n","# Load the extra large version of YOLOv8\n","model_x = YOLO('yolov8x.pt')\n","\n","# Example of how to use a different model (uncomment the line above to load it)\n","model = model_x # For example, if you want to use the small model\n"],"metadata":{"id":"ts_GFqAg2vNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: run ls -la but display file sizes in MG / GB\n","\n","!ls -lah"],"metadata":{"id":"1mSsZLsN2khj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run prediction"],"metadata":{"id":"rXbmaDZ603pk"}},{"cell_type":"code","source":["# Path to your image\n","image_path = '/content/photo_2025-07-13 11.06.57.jpeg'\n","\n","# Run inference on the image\n","results = model(image_path)\n","\n","# The 'results' object contains the detections.\n","# We can visualize them directly.\n","# The plot() method returns a NumPy array of the image with detections.\n","annotated_image = results[0].plot()\n","\n"],"metadata":{"id":"mhrX29GNz_0r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Review the rqs results"],"metadata":{"id":"kkfNgSmG3CqA"}},{"cell_type":"code","source":["results"],"metadata":{"id":"1V1FLKqL1D-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This output from YOLOv8 describes **two objects** that your model detected in an image. Both objects were identified as belonging to the **same class (class `0`)**, but with low confidence scores.\n","\n","Here is a breakdown of what each attribute means.\n","\n","***\n","\n","### Core Detection Information\n","\n","* **`shape: torch.Size([2, 6])`**\n","    This tells you the dimensions of the main results tensor. It means there are **2** detected objects, and for each object, there are **6** associated values.\n","\n","* **`data`**: `tensor([[333.52, 1083.7, 354.98, 1142.0, 0.39918, 0.0], [361.72, 1082.8, 383.89, 1152.8, 0.31925, 0.0]])`\n","    This is the raw data for the two detections. Each row `[x1, y1, x2, y2, confidence, class]` represents one object.\n","    * **Detection 1**: Bounding box from `(333, 1083)` to `(355, 1142)`, confidence `0.399`, class `0`.\n","    * **Detection 2**: Bounding box from `(361, 1082)` to `(383, 1152)`, confidence `0.319`, class `0`.\n","\n","* **`cls: tensor([0., 0.])`**\n","    This tensor lists the **class index** for each of the two detected objects. Both were classified as `0`. The class index `0` typically corresponds to the first class name in your dataset's `.yaml` file (e.g., 'person').\n","\n","* **`conf: tensor([0.3992, 0.3192])`**\n","    This shows the **confidence score** for each detection.\n","    * The first object has a confidence of **~40%**.\n","    * The second object has a confidence of **~32%**.\n","    These are relatively low scores, suggesting the model is not very certain about these detections.\n","\n","***\n","\n","### Bounding Box Formats ðŸ“¦\n","\n","YOLO provides the bounding box coordinates in multiple convenient formats. All values are in pixels unless they end with 'n' (normalized).\n","\n","* **`xyxy`**: `tensor([[ 333.52, 1083.67, 354.97, 1142.00], [ 361.71, 1082.81, 383.89, 1152.81]])`\n","    The bounding box coordinates in `[x_min, y_min, x_max, y_max]` format. `(x_min, y_min)` is the top-left corner and `(x_max, y_max)` is the bottom-right corner.\n","\n","* **`xywh`**: `tensor([[ 344.24, 1112.83, 21.45, 58.32], [ 372.80, 1117.81, 22.17, 70.00]])`\n","    The bounding box in `[x_center, y_center, width, height]` format.\n","\n","* **`xyxyn`** and **`xywhn`**:\n","    These are the **normalized** versions of the formats above. The coordinates are scaled to a range of `[0, 1]` by dividing them by the original image dimensions. This makes them independent of the image resolution.\n","\n","***\n","\n","### Image & Tracking Information\n","\n","* **`orig_shape: (1280, 960)`**\n","    The dimensions of the original input image: **1280 pixels in height** and **960 pixels in width**.\n","\n","* **`id: None`** and **`is_track: False`**\n","    These attributes are used for object tracking. Since `id` is `None`, it means you ran a standard prediction, not object tracking. The `id` would otherwise assign a unique, consistent ID to each object across multiple frames."],"metadata":{"id":"uMALVq0u2b6J"}},{"cell_type":"code","source":["results[0].boxes"],"metadata":{"id":"VJAhMd9k1Kx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["annotated_image"],"metadata":{"id":"XgtF8WWP1Bqc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Display the image"],"metadata":{"id":"rmr_mz2w3HMf"}},{"cell_type":"code","source":["\n","from IPython.display import Image, display\n","# Convert the annotated image (NumPy array) to a format displayable in Colab\n","# OpenCV images are BGR, convert to RGB for PIL/display\n","annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n","\n","# Use PIL to save the image to a BytesIO object and then display it\n","from PIL import Image as PILImage\n","import io\n","\n","pil_img = PILImage.fromarray(annotated_image_rgb)\n","byte_arr = io.BytesIO()\n","pil_img.save(byte_arr, format='PNG') # Or 'JPEG'\n","display(Image(byte_arr.getvalue()))"],"metadata":{"id":"X_aFqXoP1fbK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-jp7Q2W91ohs"},"execution_count":null,"outputs":[]}]}